#!/usr/bin/env python3
"""
Script de diagn√≥stico para problemas con Diffusers y GPU
"""

import sys
import torch
import subprocess

print("=== üîç DIAGN√ìSTICO DE SISTEMA ===\n")

# 1. Verificar PyTorch y CUDA
print("1. PyTorch y CUDA:")
print(f"   PyTorch version: {torch.__version__}")
print(f"   CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   CUDA version: {torch.version.cuda}")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print()

# 2. Verificar librer√≠as instaladas
print("2. Librer√≠as instaladas:")
libs = ['diffusers', 'transformers', 'accelerate', 'xformers']
for lib in libs:
    try:
        module = __import__(lib)
        version = getattr(module, '__version__', 'unknown')
        print(f"   {lib}: {version}")
    except ImportError:
        print(f"   {lib}: NO INSTALADO")
print()

# 3. Test b√°sico de Diffusers
print("3. Test b√°sico de Diffusers:")
try:
    from diffusers import DiffusionPipeline
    print("   ‚úÖ Diffusers importado correctamente")
    
    # Intentar cargar un modelo peque√±o
    print("   Cargando modelo de prueba...")
    pipe = DiffusionPipeline.from_pretrained(
        "hf-internal-testing/tiny-stable-diffusion-pipe",
        torch_dtype=torch.float16
    )
    pipe = pipe.to("cuda")
    print("   ‚úÖ Modelo cargado en GPU")
    
    # Test de inferencia
    print("   Ejecutando inferencia de prueba...")
    with torch.no_grad():
        result = pipe("test", num_inference_steps=1)
    print("   ‚úÖ Inferencia exitosa")
    
except Exception as e:
    print(f"   ‚ùå Error: {e}")
print()

# 4. Verificar configuraci√≥n de torch._dynamo
print("4. Configuraci√≥n de torch._dynamo:")
try:
    import torch._dynamo
    print(f"   suppress_errors: {torch._dynamo.config.suppress_errors}")
    
    # Configurar para suprimir errores
    torch._dynamo.config.suppress_errors = True
    print("   ‚úÖ suppress_errors configurado a True")
except Exception as e:
    print(f"   ‚ùå Error con torch._dynamo: {e}")
print()

# 5. Test de modelos espec√≠ficos
print("5. Test de modelos espec√≠ficos:")
models_to_test = [
    ("SimianLuo/LCM_Dreamshaper_v7", "LCM"),
    ("stabilityai/sdxl-turbo", "SDXL-Turbo"),
]

for model_id, name in models_to_test:
    print(f"\n   Testing {name}...")
    try:
        # Solo intentar importar, no cargar completamente
        from diffusers import AutoPipelineForImage2Image
        print(f"   ‚úÖ {name} disponible")
    except Exception as e:
        print(f"   ‚ùå {name} error: {e}")

print("\n6. Recomendaciones:")
print("   - Si hay errores con accelerate, intenta: pip uninstall accelerate && pip install accelerate==0.25.0")
print("   - Si hay errores con torch.compile, usa torch._dynamo.config.suppress_errors = True")
print("   - Para m√°ximo rendimiento, usa SDXL-Turbo o LCM sin cpu_offload")
print("   - Si persisten los errores, usa la versi√≥n minimal sin optimizaciones avanzadas")

print("\n=== üèÅ DIAGN√ìSTICO COMPLETO ===")

# 7. Crear archivo de configuraci√≥n recomendado
print("\nCreando archivo config_recommended.py...")
config_content = """# Configuraci√≥n recomendada para m√°ximo rendimiento

import torch
import torch._dynamo

# Suprimir errores de compilaci√≥n
torch._dynamo.config.suppress_errors = True

# Configuraci√≥n para mejor rendimiento
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# Modelo recomendado para velocidad
MODEL_ID = "stabilityai/sdxl-turbo"  # o "SimianLuo/LCM_Dreamshaper_v7"
TORCH_DTYPE = torch.float16

# Configuraci√≥n de inferencia
INFERENCE_STEPS = 1  # SDXL-Turbo solo necesita 1
GUIDANCE_SCALE = 0.0  # Sin guidance para m√°xima velocidad
STRENGTH = 0.3  # Menor = m√°s r√°pido y m√°s parecido al original

print("‚úÖ Configuraci√≥n cargada")
"""

with open("config_recommended.py", "w") as f:
    f.write(config_content)

print("‚úÖ Archivo config_recommended.py creado")
print("\nPara usar esta configuraci√≥n, importa al inicio de tu servidor:")
print("   from config_recommended import *")
